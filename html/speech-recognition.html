

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Speech Recognition &mdash; OpenSeq2Seq 0.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_override.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_override.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="DeepSpeech2" href="speech-recognition/deepspeech2.html" />
    <link rel="prev" title="Optimizers" href="optimizers.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> OpenSeq2Seq
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-existing-models.html">Using Existing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="distr-training.html">Multi-GPU and Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed-precision.html">Mixed Precision Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizers.html">Optimizers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Speech Recognition</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#models">Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speech-recognition/deepspeech2.html">DeepSpeech2</a></li>
<li class="toctree-l3"><a class="reference internal" href="speech-recognition/wave2letter.html">Wave2Letter+</a></li>
<li class="toctree-l3"><a class="reference internal" href="speech-recognition/jasper.html">Jasper</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#decoders">Decoders</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting started</a></li>
<li class="toctree-l2"><a class="reference internal" href="#synthetic-data">Synthetic data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speech-recognition/synthetic_dataset.html">Creation of Synthetic Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="speech-recognition/synthetic_dataset.html#training-with-synthetic-data">Training With Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tools">Tools</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="speech-commands.html">Speech Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech-synthesis.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="machine-translation.html">Machine Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="language-model.html">Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="sentiment-analysis.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="image-classification.html">Image Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="interactive-infer-demos.html">Interactive Infer Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="adding-new-models.html">Adding new models</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-docs/modules.html">API documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenSeq2Seq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Speech Recognition</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/speech-recognition.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="speech-recognition">
<span id="id1"></span><h1>Speech Recognition<a class="headerlink" href="#speech-recognition" title="Permalink to this headline">¶</a></h1>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<p>Currently we support following models:</p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="33%" />
<col width="17%" />
<col width="33%" />
<col width="17%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Model description</th>
<th class="head">Greedy WER, %</th>
<th class="head">Config file</th>
<th class="head">Checkpoint</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference internal" href="speech-recognition/jasper.html"><span class="doc">Jasper DR 10x5</span></a></td>
<td>3.61</td>
<td><a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/speech2text/jasper10x5_LibriSpeech_nvgrad_masks.py">jasper10x5_LibriSpeech_nvgrad_masks</a></td>
<td><a class="reference external" href="https://drive.google.com/open?id=12CQvNrTvf0cjTsKjbaWWvdaZb7RxWI6X">link</a></td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="speech-recognition/wave2letter.html"><span class="doc">Wave2Letter+</span></a></td>
<td>6.67</td>
<td><a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/speech2text/w2lplus_large_8gpus_mp.py">w2l_plus_large_mp</a></td>
<td><a class="reference external" href="https://drive.google.com/file/d/10EYe040qVW6cfygSZz6HwGQDylahQNSa/view?usp=sharing">link</a></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="speech-recognition/deepspeech2.html"><span class="doc">DeepSpeech2</span></a></td>
<td>6.71</td>
<td><a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/speech2text/ds2_large_8gpus_mp.py">ds2_large_mp</a></td>
<td><a class="reference external" href="https://drive.google.com/open?id=1EDvL9wMCO2vVE-ynBvpwkFTultbzLNQX">link</a></td>
</tr>
</tbody>
</table>
<p>WER is the <a class="reference external" href="https://en.wikipedia.org/wiki/Word_error_rate">word error rate</a> obtained on a dev-clean subset of LibriSpeech using
greedy decoder (<code class="docutils literal notranslate"><span class="pre">decoder_params/use_language_model</span> <span class="pre">=</span> <span class="pre">False</span></code>).
For the evaluation we used <code class="docutils literal notranslate"><span class="pre">batch_size_per_gpu</span> <span class="pre">=</span> <span class="pre">1</span></code>
to eliminate the effect of <a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/issues/69">cuDNN padding issue</a>.</p>
<p>For more details about model and training parameters,
have a look at the <a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/speech2text">configuration files</a> and specific model’s documentation.</p>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Automatic speech recognition (ASR) systems can be built using a number of approaches depending on input data type, intermediate representation, model’s type and output post-processing.
OpenSeq2Seq is currently focused on end-to-end CTC-based models (like original DeepSpeech model). These models are called end-to-end because they take speech samples and transcripts without any additional information. CTC allows finding an alignment between audio and text. CTC ASR models can be summarized in the following scheme:</p>
<img alt="CTC ASR models" class="align-center" src="_images/ctc_asr.png" />
<p>Training pipeline consists of the following blocks:</p>
<ol class="arabic simple">
<li>audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)</li>
<li>neural acoustic model (which predicts a probability distribution <em>P_t(c)</em> over vocabulary characters <em>c</em> per each time step <em>t</em> given input features per each timestep)</li>
<li>CTC loss function</li>
</ol>
<p>Inference pipeline is different for block #3:</p>
<ol class="arabic simple" start="3">
<li>decoder (which transforms a probability distribution into actual transcript)</li>
</ol>
<p>We support different options for these steps.
The recommended pipeline is the following (in order to get the best accuracy, the lowest WER):</p>
<ol class="arabic simple">
<li>Mel scale log spectrograms for audio features (using <cite>librosa</cite> backend)</li>
<li>Jasper as a neural acoustic model</li>
<li>Baidu’s CTC beam search decoder with N-gram language model rescoring</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>OpenSeq2Seq has two audio feature extraction backends:</p>
<ol class="arabic simple">
<li><cite>python_speech_features</cite> (<cite>psf</cite>, it is a default backend for backward compatibility)</li>
<li><cite>librosa</cite></li>
</ol>
<p class="last">We recommend to use <cite>librosa</cite> backend for its numerous important features (e.g., windowing, more accurate mel scale aggregation).
To enable <cite>librosa</cite>, please make sure that there is a line <code class="docutils literal notranslate"><span class="pre">&quot;backend&quot;:</span> <span class="pre">&quot;librosa&quot;</span></code> in <code class="docutils literal notranslate"><span class="pre">&quot;data_layer_params&quot;</span></code>.</p>
</div>
<div class="section" id="decoders">
<h3>Decoders<a class="headerlink" href="#decoders" title="Permalink to this headline">¶</a></h3>
<p>In order to get words out of a trained model one needs to use a decoder. Decoder converts a probability distribution over characters into text. There are two types of decoders that are usually employed with CTC-based models: greedy decoder and beam search decoder with language model re-scoring.</p>
<p>A greedy decoder outputs the most probable character at each time step. It is very fast and it can produce transcripts that are very close to the original pronunciation. But it may introduce many small misspelling errors. Due to the nature of WER metric, even one character error makes a whole word incorrect.</p>
<p>A beam search decoder with language model re-scoring allows checking many possible decodings (beams) at once with assigning a higher score for more probable N-grams according to a given language model. The language model helps to correct misspelling errors. The downside is that it is significantly slower than a greedy decoder.</p>
<p>There are two implementations of beam search decoder in OpenSeq2Seq:</p>
<ol class="arabic">
<li><p class="first">native TensorFlow operation (<code class="docutils literal notranslate"><span class="pre">./ctc_decoder_with_lm/</span></code>). It is rather a deprecated decoder due to its slowness (it works in a single CPU thread only). We keep it for backward compatibility. You have to build it (or use pre-built version in NVIDIA TensorFlow container). In order to enable it, you’ll need to define its parameters <code class="docutils literal notranslate"><span class="pre">&quot;beam_width&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;alpha&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;beta&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;decoder_library_path&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;lm_path&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;trie_path&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;alphabet_config_path&quot;</span></code> and add <code class="docutils literal notranslate"><span class="pre">&quot;use_language_model&quot;:</span> <span class="pre">True</span></code> line in <code class="docutils literal notranslate"><span class="pre">&quot;decoder_params&quot;</span></code> section of the config file.</p>
</li>
<li><p class="first">Baidu decoder (as a separate Python script). It is parallelized across batch on multiple CPU cores, so it is significantly faster. It doesn’t require a separate trie file as an input. It is the recommended decoder for ASR models. In order to use it, please:</p>
<ul>
<li><p class="first">make sure that <code class="docutils literal notranslate"><span class="pre">&quot;decoder_params&quot;</span></code> section has <code class="docutils literal notranslate"><span class="pre">'infer_logits_to_pickle':</span> <span class="pre">True</span></code> line and that <code class="docutils literal notranslate"><span class="pre">&quot;dataset_files&quot;</span></code> field of <code class="docutils literal notranslate"><span class="pre">&quot;infer_params&quot;</span></code> section contains a target CSV file</p>
</li>
<li><p class="first">run inference (to dump logits to a pickle file):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="n">infer</span> <span class="o">--</span><span class="n">config</span><span class="o">=</span><span class="s2">&quot;MODEL_CONFIG&quot;</span> <span class="o">--</span><span class="n">logdir</span><span class="o">=</span><span class="s2">&quot;MODEL_CHECKPOINT_DIR&quot;</span> <span class="o">--</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span> <span class="o">--</span><span class="n">use_horovod</span><span class="o">=</span><span class="kc">False</span> <span class="o">--</span><span class="n">decoder_params</span><span class="o">/</span><span class="n">use_language_model</span><span class="o">=</span><span class="kc">False</span> <span class="o">--</span><span class="n">infer_output_file</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">pickle</span>
</pre></div>
</div>
</li>
<li><p class="first">run beam search decoder (with specific ALPHA, BETA and BEAM_WIDTH hyperparameters):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">decode</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">logits</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">pickle</span> <span class="o">--</span><span class="n">labels</span><span class="o">=</span><span class="s2">&quot;CSV_FILE&quot;</span> <span class="o">--</span><span class="n">lm</span><span class="o">=</span><span class="s2">&quot;LM_BINARY&quot;</span>  <span class="o">--</span><span class="n">vocab</span><span class="o">=</span><span class="s2">&quot;ALPHABET_FILE&quot;</span> <span class="o">--</span><span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span> <span class="o">--</span><span class="n">beta</span><span class="o">=</span><span class="n">BETA</span> <span class="o">--</span><span class="n">beam_width</span><span class="o">=</span><span class="n">BEAM_WIDTH</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
<p>It is possible to apply a neural language model (like Transformer-XL) to select the best trascription among all candidates after the beam search. For more details, please see <a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/tree/master/external_lm_rescore">https://github.com/NVIDIA/OpenSeq2Seq/tree/master/external_lm_rescore</a></p>
</div>
</div>
<div class="section" id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<p>You can start with <a class="reference internal" href="speech-recognition/get_started_toy_model.html"><span class="doc">these instructions</span></a>
to play with a very small model on a toy dataset.</p>
<p>Now let’s consider a relatively lightweight version of DeepSpeech2 based model for
English speech recognition on LibriSpeech dataset.</p>
<p>Download and preprocess LibriSpeech dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">import_librivox</span><span class="o">.</span><span class="n">py</span> <span class="n">data</span><span class="o">/</span><span class="n">librispeech</span>
</pre></div>
</div>
<p>Install KenLM:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scripts</span><span class="o">/</span><span class="n">install_kenlm</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>Download and preprocess OpenSLR language model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scripts</span><span class="o">/</span><span class="n">download_lm</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>Let’s train a small DS2 model.</p>
<p>This model can be trained on 12 GB GPU within a day.</p>
<p>Start training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=</span><span class="n">example_configs</span><span class="o">/</span><span class="n">speech2text</span><span class="o">/</span><span class="n">ds2_small_1gpu</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="n">train_eval</span>
</pre></div>
</div>
<p>If your GPU does not have enough memory, reduce the <code class="docutils literal notranslate"><span class="pre">batch_size_per_gpu</span></code>.
Also, you might want to disable evaluation during training by using <code class="docutils literal notranslate"><span class="pre">--mode=train</span></code>.</p>
<p>In order to get greedy WER metric on validation dataset, please run the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=</span><span class="n">example_configs</span><span class="o">/</span><span class="n">speech2text</span><span class="o">/</span><span class="n">ds2_small_1gpu</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="nb">eval</span>
</pre></div>
</div>
<p>If you would like to use beam search decoder with language model re-scoring, please see <a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/tree/master/external_lm_rescore">link</a></p>
<p>Once training is done (this can take a while on a single GPU), you can run inference:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=</span><span class="n">example_configs</span><span class="o">/</span><span class="n">speech2text</span><span class="o">/</span><span class="n">ds2_small_1gpu</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="n">infer</span> <span class="o">--</span><span class="n">infer_output_file</span><span class="o">=</span><span class="n">ds2_out</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>To train on &lt;N&gt; GPUs without Horovod:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=...</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="n">train_eval</span> <span class="o">--</span><span class="n">use_horovod</span><span class="o">=</span><span class="kc">False</span> <span class="o">--</span><span class="n">num_gpus</span><span class="o">=&lt;</span><span class="n">N</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>To train with Horovod on &lt;N&gt; GPUs, use the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mpiexec</span> <span class="o">--</span><span class="n">allow</span><span class="o">-</span><span class="n">run</span><span class="o">-</span><span class="k">as</span><span class="o">-</span><span class="n">root</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">N</span><span class="o">&gt;</span> <span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=...</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="n">train_eval</span> <span class="o">--</span><span class="n">use_horovod</span><span class="o">=</span><span class="kc">True</span>
</pre></div>
</div>
</div>
<div class="section" id="synthetic-data">
<h2>Synthetic data<a class="headerlink" href="#synthetic-data" title="Permalink to this headline">¶</a></h2>
<p>Speech recognition models can be optionally trained using synthetic data.
The creation of the synthetic data and training process is described <a class="reference internal" href="speech-recognition/synthetic_dataset.html#synthetic-data"><span class="std std-ref">here</span></a>.</p>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="tools">
<h2>Tools<a class="headerlink" href="#tools" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference internal" href="speech-recognition/speech-to-text-align.html"><span class="doc">A tool for generating time stamps per each word</span></a></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="speech-recognition/deepspeech2.html" class="btn btn-neutral float-right" title="DeepSpeech2" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="optimizers.html" class="btn btn-neutral" title="Optimizers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>  
  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #64d81c;
    }
    .wy-side-nav-search > div.version {
      color: #ffffff;
    }
    .wy-side-nav-search > img {
      max-width: 150px;
    }
    .wy-side-nav-search > a {
      font-size: 23px;
    }
  </style>


</body>
</html>